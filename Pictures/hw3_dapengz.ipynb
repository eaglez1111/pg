{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 10701_hw3_student",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXADdUMUccUM",
        "colab_type": "text"
      },
      "source": [
        "# 10701 HW3: Implementing Neural Network\n",
        "In this part, you will implement the basic layers and the backpropagation algorithm for a neural network. We are not considering convolution layers or any fancy techniques like batch normalization. In the end, you will train a multi-layered, fully connected neural network to recognize digits from handwritten images, specifically speaking, the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), with validation error rate less than 4%.\n",
        "\n",
        "![MNIST_example](https://drive.google.com/uc?export=view&id=1q4Kjt_OhiC1TMLq_pDtmwukxvKJGKBs5)\n",
        "\n",
        "This homework is beneficial in that you will have a solid understanding of the mathematical derivation and practical programming of simple neural networks, which means that you are **NOT** allowed to use any 3rd party external libraries (PyTorch, TensorFlow etc.) other than Numpy for linear algebra and Matplotlib for plotting.\n",
        "\n",
        "You are recommended to go over related lectures before starting. If you are ready, let's get started. \n",
        "\n",
        "---\n",
        "\n",
        "Note: There is always a **TODO** sign where you should add your code. Please do not modify anywhere else. \\\\\n",
        "Note: To help you debug, we have provided you with the sample output of the first 10 epochs of training and the resulting plot after 100 epochs of training. When grading, we will run your code to check if it produces the correct output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvkOq7NPnlwX",
        "colab_type": "text"
      },
      "source": [
        "## Basic layers\n",
        "\n",
        "As you learned from the lecture, neural networks are composed of basic layers. Let's first take a look at the base class of layers. \n",
        "\n",
        "Each layer basically contains a *`forward`* and a *`backward`* member function. The *`forward`* function takes an input, makes some modifications and returns an output. It may also keep some records in its internal states necessary for the later backpropagation. The *`backward`* function takes a *delta* from the next layer and a *learning rate*, which are used to calculate the delta as to the last layer and update its own parameters. All the basic layers inherit from the *`Layer`* class and should implement their own *`forward`* and a *`backward`* functions. Here we will implement two of the basic layers: ReLU layer and linear layer (also known as fully connected layer). \n",
        "\n",
        "Note: We will train the neural network with Mini-batch gradient descent. The $batch\\_size$ is the number of training samples fed into the network each time, and each sample may have $n\\_features$ features. Thus the input to each layer is a 2-d matrix of size $(batch\\_size, n\\_features)$ where each row represents a sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKDZ3s1TpAO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do not import any additional 3rd party external libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    \"\"\"\n",
        "    Base class for neural network layers\n",
        "    \"\"\"\n",
        "    def __call__(self, x):\n",
        "        # Syntactic sugar. Overrides the function call operator.\n",
        "        # If l is an instance of Layer, then l(x) == l.forward(x)\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError  # do NOT modify\n",
        "\n",
        "    def backward(self, delta, learning_rate):\n",
        "        raise NotImplementedError  # do NOT modify"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ADMhoLcmYdd",
        "colab_type": "text"
      },
      "source": [
        "## ReLU layer\n",
        "\n",
        "$$y=ReLU(x)=\\max(0,x)$$\n",
        "\n",
        "The rectified linear unit function is a simple non-linear activation function, which keeps the positive part of the input and sets negative part to zero. The value of the derivative at 0 can be arbitrarily set to 0 or 1.\n",
        "\n",
        "Hint:\n",
        "Basically, we have $delta = \\frac{\\partial loss}{\\partial y}$ and $y = ReLU(x)$. The *`backward`* function should return $\\frac{\\partial loss}{\\partial x}$ as the $delta$ to the last layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HCLht05oNZ3",
        "colab_type": "code",
        "outputId": "0218c806-75e8-4da8-cc2c-b6a998b01028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class ReLU(Layer):\n",
        "    \"\"\"\n",
        "    Applies the rectified linear unit function element-wise\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # initialize any internal variables\n",
        "        # TODO: initialize internal viriable as needed\n",
        "        self.x = np.array([])\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, n_features)\n",
        "        # return: (batch_size,n_features)\n",
        "        # TODO: apply ReLU function\n",
        "        self.x = x\n",
        "        return np.maximum(0,x)\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, delta, learning_rate):\n",
        "        # delta: (batch_size, n_features)\n",
        "        # return: (batch_size, n_features)\n",
        "        # TODO: return delta for last layer\n",
        "        return delta*(self.x>0).astype(int)\n",
        "        raise NotImplementedError\n",
        "\n",
        "def testReLU():\n",
        "    relu = ReLU()\n",
        "\n",
        "    x = np.array([[-1, 1]])\n",
        "    expected = np.array([[0, 1]])\n",
        "    assert (relu(x)==expected).all(), \"check your forward function\"\n",
        "\n",
        "    delta = np.array([[10, -10]])\n",
        "    expected = np.array([[0, -10]])\n",
        "    assert (relu.backward(delta, 1e-4)==expected).all(), \"check your backward function\"\n",
        "\n",
        "    print(\"testReLU passed!\")\n",
        "\n",
        "testReLU()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testReLU passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQdaxMh_qvD3",
        "colab_type": "text"
      },
      "source": [
        "## Linear layer\n",
        "\n",
        "$$y=xW+b$$\n",
        "\n",
        "Linear layer applies a linear transformation to the input data. $x$ is a sample row vector of length $n\\_in$. $W$ is a matrix of size $(n\\_in, n\\_out)$ and b is the bias term of length $n\\_out$. $W$ and $b$ are shared among all the samples. You can initialize each entry of $W$ with standard Gaussian distribution and $b$ with a zero vector.\n",
        "\n",
        "$$W \\leftarrow W - \\eta \\frac{\\partial loss}{\\partial W}, b \\leftarrow b - \\eta \\frac{\\partial loss}{\\partial b}$$\n",
        "\n",
        "To apply backpropagation, follow the update rules above where $\\eta$ is the learning rate. The return value should be $\\frac{\\partial loss}{\\partial x}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFdOBPlnca8f",
        "colab_type": "code",
        "outputId": "aa6b9098-5a6a-458a-b811-04f18a28f07e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class Linear(Layer):\n",
        "    \"\"\"\n",
        "    Applies a linear transformation to the input\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        # n_in: feature size of the input samples\n",
        "        # n_out: output size of each sample\n",
        "        # TODO: initialize internal viriable as needed\n",
        "        # Please put your parameters as self.W and self.b for testLinear()\n",
        "        self.W = np.random.normal(0,1,[n_in,n_out])\n",
        "        self.b = np.zeros(n_out,dtype='float32')\n",
        "        self.x = np.array([])\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, n_in)\n",
        "        # return: (batch_size, n_out)\n",
        "        # TODO: return the result of linear transformation\n",
        "        self.x = x\n",
        "        return x.dot(self.W) + self.b\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, delta, learning_rate):\n",
        "        # delta: (batch_size, n_out)\n",
        "        # return: (batch_size, n_in)\n",
        "        # TODO: update internal parameters and return delta for last layer\n",
        "        result = delta.dot(self.W.T)\n",
        "        delta *= learning_rate\n",
        "        self.W = self.W - self.x.T.dot(delta)\n",
        "        self.b = self.b - np.ones([1,len(delta)]).dot(delta)\n",
        "        return result\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "def testLinear():\n",
        "    linear = Linear(2, 3)\n",
        "    # hardcoded parameters\n",
        "    linear.W = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "    linear.b = np.array([7, 8, 9])\n",
        "\n",
        "    x = np.array([[0, 1]])\n",
        "    expected = np.array([[11, 13, 15]])\n",
        "    assert (linear(x)==expected).all(), \"check your forward function\"\n",
        "\n",
        "    delta = np.array([[1, 2, 3]])\n",
        "    expected = np.array([[14, 32]])\n",
        "    assert (linear.backward(delta, 1)==expected).all(), \"return value is wrong, check your backward function\"\n",
        "    expected = np.array([[1, 2, 3], [3, 3, 3]])\n",
        "    assert (linear.W==expected).all(), \"W is wrong, check your backward function\"\n",
        "    expected = np.array([[6, 6, 6]])\n",
        "    assert (linear.b==expected).all(), \"b is wrong, check your backward function\"\n",
        "\n",
        "    print(\"testLinear passed!\")\n",
        "\n",
        "testLinear()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testLinear passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8gITTYz4tJ6",
        "colab_type": "text"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "Loss is an operation which takes a prediction and a target and outputs how much different they are in a particular metric. In class we have learned Mean Square Error Loss (MSELoss) for regression jobs. Since we want to classify an image into a digit, we need another loss function for this classification job: Cross Entropy Loss. Let's first look at the base class for the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcCGuKef6cjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Loss:\n",
        "    \"\"\"\n",
        "        Base class for neural network loss\n",
        "    \"\"\"\n",
        "    def __call__(self, x, y):\n",
        "        # Syntactic sugar. Overrides the function call operator.\n",
        "        # If l is an instance of Loss, then l(x, y) == l.forward(x, y)\n",
        "        return self.forward(x, y)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x is the prediction, y is the target\n",
        "        raise NotImplemented  # do NOT modify\n",
        "\n",
        "    def backward(self):\n",
        "        # return delta for last layer\n",
        "        raise NotImplemented  # do NOT modify"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ooQg_yh1zLW",
        "colab_type": "text"
      },
      "source": [
        "## Cross entropy loss\n",
        "\n",
        "$$CrossEntropyLoss(x, label)=-\\log(\\frac{\\exp(x[label])}{\\sum_i{\\exp(x[i])}})$$\n",
        "\n",
        "Cross entropy loss is useful for classification jobs. Say $x$ is a feature vector of a sample of length $n\\_classes$ and $label \\in [0, n\\_classes-1]$ is an integer. Basically the loss computes the softmax of the input and returns its negative log likelihood of the target $label$ term.\n",
        "\n",
        "Hint: \n",
        "1. Cross entropy loss has two steps. You can use the chain rule to compute $\\frac{\\partial loss}{\\partial x}$.\n",
        "2. $\\log(x)$ is problematic when $x\\to 0$. Similarly $\\exp(x)$ may overflow when $x$ is huge. Think of some way to refrain from the issues.\n",
        "3. *`for`* loop in Python is slow. Consider native Numpy functions when possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJJx8oTi4kcs",
        "colab_type": "code",
        "outputId": "b6756f23-0110-47e1-a86b-e76c30a83876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class CrossEntropyLoss(Loss):\n",
        "    \"\"\"\n",
        "    Cross entropy loss.\n",
        "    Combines softmax and negative log likelihood.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # TODO: initialize internal viriable as needed\n",
        "        self.y_idx = np.array([[]])\n",
        "        self.o = np.array([[]])\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: (batch_size, n_classes)\n",
        "        # y: (batch_size,), each item within [0, n_classes-1]\n",
        "        # return: (2,), sum of loss and sum of accuracy of all samples\n",
        "        # TODO: finish the forward function\n",
        "        accu = (x.argmax(1)==y).astype(int) \n",
        "        \n",
        "        batch_size = len(y)\n",
        "        self.y_idx = (range(batch_size),y)\n",
        "        \n",
        "        x -= np.array([x.max(1)]).T\n",
        "        x = np.exp(x)\n",
        "        self.o = x/np.array([x.sum(1)]).T\n",
        "        loss = -np.log(self.o[self.y_idx])        \n",
        "        \n",
        "        return [np.sum(loss),np.sum(accu)]\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        # return: (batch_size, n_classes)\n",
        "        # TODO: return delta for last layer\n",
        "        d = self.o\n",
        "        d[self.y_idx] = d[self.y_idx]-1\n",
        "        return d\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "def testCELoss():\n",
        "    loss = CrossEntropyLoss()\n",
        "    \n",
        "    x = np.array([[0, 1, 2]])\n",
        "    y = np.array([1])\n",
        "    expected_loss = 1.4076\n",
        "    sum_loss, sum_acc = loss(x, y)\n",
        "    assert (np.abs(sum_loss-expected_loss)<1e-3).all(), \"check loss of your forward function\"\n",
        "    assert sum_acc == 0, \"check accuracy of your forward function\"\n",
        "    \n",
        "    expected = np.array([[0.0900, -0.7553, 0.6652]])\n",
        "    assert (np.abs(loss.backward()-expected)<1e-3).all(), \"check your backward function\"\n",
        "\n",
        "    print(\"testCELoss passed!\")\n",
        "\n",
        "testCELoss()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testCELoss passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekmd70i4TqCk",
        "colab_type": "text"
      },
      "source": [
        "## Neural network\n",
        "\n",
        "Finally, you need to put all the layers together to form an interesting and functional neural network. Here is the NN class that is able to take different configurations of layers and a loss criterion. \n",
        "\n",
        "During training, it updates the parameter of layers based on a number of samples, which is called Mini-Batch Gradient Descent. The number is termed $batch\\_size$. When $batch\\_size=1$, it's degraded to Stochastic Gradient Descent (SGD). When $batch\\_size$ equals the number of all training samples, it's degraded to Batch Gradient Descent (BGD).\n",
        "\n",
        "Now follow the comments to finish the NN class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs_kEg53T16J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN:\n",
        "    \"\"\"\n",
        "    A simple multilayer neural network\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, criterion):\n",
        "        # layers: a list of Layer instances\n",
        "        # criterion: a Loss instance\n",
        "        self.layers = layers\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def train(self, x, y, learning_rate):\n",
        "        # train on a batch of samples, output sum of loss and sum of accuracy, update parameters\n",
        "        # x: (batch_size, n_features)\n",
        "        # y: (batch_size,), each item within [0, n_class-1]\n",
        "        # return: loss, accuracy\n",
        "\n",
        "        # TODO: pass x through layers one by one\n",
        "        n_layers = len(self.layers)\n",
        "        for i in range(n_layers):\n",
        "          x = self.layers[i](x)\n",
        "        \n",
        "        # TODO: get loss and accuracy by the criterion\n",
        "        loss, accuracy = self.criterion(x,y)\n",
        "        \n",
        "        # TODO: back propagation and update parameters\n",
        "        if not (learning_rate == 0):\n",
        "          delta = self.criterion.backward()\n",
        "          for i in range(n_layers):\n",
        "            delta = self.layers[n_layers-i-1].backward(delta,learning_rate)\n",
        "          \n",
        "        return loss, accuracy\n",
        "\n",
        "    def val(self, x, y):\n",
        "        # validate on a batch of samples, output sum of loss and sum of accuracy, no parameter update\n",
        "        # x: (batch_size, n_features)\n",
        "        # y: (batch_size,), each item within [0, n_class-1]\n",
        "        # return: loss, accuracy\n",
        "\n",
        "        # TODO: get loss and accuracy\n",
        "        loss, accuracy = self.train(x,y,0)\n",
        "        \n",
        "        return loss, accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l3L2COKds_C",
        "colab_type": "text"
      },
      "source": [
        "## Training the neural network\n",
        "\n",
        "There are two ways you can download and import MNIST dataset:\n",
        "\n",
        "1. First download the MNIST dataset [here](https://drive.google.com/file/d/1UC_uf492dgNfPQSmQa7dMFj8R2502dGb/view?usp=sharing) and upload the npz file to the file system on the left panel. The npz file contains training/validation data/labels in numpy data format. The 2d images (28*28) have been flattened to 1d features (784). The data has been normalized to [0, 1] for easier training. Thanks to Yann LeCun and his colleagues for making it public on his [website](http://yann.lecun.com/exdb/mnist/).\n",
        "2. Use *Keras.datasets* to import raw mnist data. Then, you could reshape and normalize to make it same as the dataset downloaded from (1).\n",
        "\n",
        "Specifically, the training set has 60k samples and the validation set has 10k samples. The neural network will be trained on the training set and the accuracy on the validation set reflects how well it learns and its generalization ability. \n",
        "\n",
        "One epoch means it has finished training on the training set and usually it needs many epochs to learn things well. There are fancy techniques of early stopping or changing learning rate dynamically, but we just fix the learning rate to $10^{-3}$ and see how it works. \n",
        "\n",
        "Note: You should expect an accuracy of at least 96% for the validation set.\n",
        "\n",
        "Note: You can play with different parameters ([link](http://yann.lecun.com/exdb/mnist/)) and learning rates to get better validation accuracy. For example, using 2-layer NN, 800 Hidden Units, Cross-Entropy Loss, one can achieve 1.6% error rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "US7Awha34how",
        "outputId": "8f787d7c-2e99-4842-c90a-82bf1adf7cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# download and import MNIST data\n",
        "# 1st method\n",
        "mnist_data = np.load(\"mnist.npz\")\n",
        "train_x, train_y = mnist_data[\"train_x\"], mnist_data[\"train_y\"]  # (60000, 784) (60000,)\n",
        "val_x, val_y = mnist_data[\"val_x\"], mnist_data[\"val_y\"]  # (10000, 784) (10000,)\n",
        "train_num, val_num = train_x.shape[0], val_x.shape[0]\n",
        "\n",
        "\"\"\"\n",
        "# 2nd method\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_x, train_y), (val_x, val_y) = mnist.load_data()\n",
        "train_x = train_x.reshape(train_x.shape[0], -1) / 255 #reshape + normalize\n",
        "val_x = val_x.reshape(val_x.shape[0], -1) / 255\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Dimension of train data: {train_x.shape}\")\n",
        "print(f\"Dimension of validation data: {val_x.shape}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension of train data: (60000, 784)\n",
            "Dimension of validation data: (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHrRHIZDd1xy",
        "colab_type": "code",
        "outputId": "c7152426-958e-4233-9af1-74e6bf1b8478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(10701) # DO NOT CHANGE THE SEED\n",
        "\n",
        "# define all training parameters\n",
        "# TODO: modify parameters as needed\n",
        "n_epochs = 100\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "layers = [\n",
        "    Linear(784, 800),\n",
        "    ReLU(),\n",
        "    Linear(800, 10)\n",
        "]\n",
        "criterion = CrossEntropyLoss()\n",
        "mnist_nn = NN(layers, criterion)\n",
        "\n",
        "\n",
        "# statistic data\n",
        "train_loss_list, train_acc_list = [], []\n",
        "val_loss_list, val_acc_list = [], []\n",
        "\n",
        "# begin training and validation\n",
        "for e in range(n_epochs):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    val_loss, val_acc = 0, 0\n",
        "\n",
        "    # shuffle the training set each epoch to prevent overfitting\n",
        "    idxs = np.arange(train_num)\n",
        "    np.random.shuffle(idxs)\n",
        "    train_x, train_y = train_x[idxs], train_y[idxs]\n",
        "\n",
        "    # training\n",
        "    for b in range(0, train_num, batch_size):\n",
        "        range_ = range(b, min(b + batch_size, train_num))\n",
        "        loss, accuracy = mnist_nn.train(train_x[range_], train_y[range_], learning_rate)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy\n",
        "\n",
        "    # validation\n",
        "    for b in range(0, val_num, batch_size):\n",
        "        range_ = range(b, min(b + batch_size, val_num))\n",
        "        loss, accuracy = mnist_nn.val(val_x[range_], val_y[range_])\n",
        "        val_loss += loss\n",
        "        val_acc += accuracy\n",
        "\n",
        "    train_loss /= train_num\n",
        "    train_acc /= train_num\n",
        "    val_loss /= val_num\n",
        "    val_acc /= val_num\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    # summary of the epoch\n",
        "    print(\"epoch: {}, train acc: {:.2f}%, train loss: {:.3f}, val acc: {:.2f}%, val loss: {:.3f}\"\n",
        "          .format(e+1, train_acc*100, train_loss, val_acc*100, val_loss))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, train acc: 87.40%, train loss: 10.339, val acc: 90.28%, val loss: 6.057\n",
            "epoch: 2, train acc: 93.35%, train loss: 3.516, val acc: 92.68%, val loss: 3.780\n",
            "epoch: 3, train acc: 95.10%, train loss: 2.132, val acc: 94.73%, val loss: 2.671\n",
            "epoch: 4, train acc: 96.12%, train loss: 1.407, val acc: 94.94%, val loss: 2.401\n",
            "epoch: 5, train acc: 96.86%, train loss: 0.994, val acc: 95.31%, val loss: 2.055\n",
            "epoch: 6, train acc: 97.30%, train loss: 0.740, val acc: 95.61%, val loss: 1.846\n",
            "epoch: 7, train acc: 97.87%, train loss: 0.519, val acc: 95.40%, val loss: 2.014\n",
            "epoch: 8, train acc: 98.10%, train loss: 0.406, val acc: 95.41%, val loss: 1.984\n",
            "epoch: 9, train acc: 98.52%, train loss: 0.294, val acc: 95.71%, val loss: 1.814\n",
            "epoch: 10, train acc: 98.71%, train loss: 0.219, val acc: 95.90%, val loss: 1.671\n",
            "epoch: 11, train acc: 98.84%, train loss: 0.181, val acc: 95.78%, val loss: 1.708\n",
            "epoch: 12, train acc: 99.10%, train loss: 0.128, val acc: 95.74%, val loss: 1.713\n",
            "epoch: 13, train acc: 99.22%, train loss: 0.096, val acc: 96.20%, val loss: 1.634\n",
            "epoch: 14, train acc: 99.34%, train loss: 0.074, val acc: 95.98%, val loss: 1.652\n",
            "epoch: 15, train acc: 99.53%, train loss: 0.051, val acc: 96.25%, val loss: 1.640\n",
            "epoch: 16, train acc: 99.66%, train loss: 0.031, val acc: 96.12%, val loss: 1.617\n",
            "epoch: 17, train acc: 99.62%, train loss: 0.034, val acc: 95.93%, val loss: 1.610\n",
            "epoch: 18, train acc: 99.70%, train loss: 0.023, val acc: 96.24%, val loss: 1.543\n",
            "epoch: 19, train acc: 99.77%, train loss: 0.019, val acc: 96.15%, val loss: 1.617\n",
            "epoch: 20, train acc: 99.87%, train loss: 0.009, val acc: 96.12%, val loss: 1.625\n",
            "epoch: 21, train acc: 99.86%, train loss: 0.009, val acc: 96.27%, val loss: 1.608\n",
            "epoch: 22, train acc: 99.87%, train loss: 0.008, val acc: 96.26%, val loss: 1.549\n",
            "epoch: 23, train acc: 99.92%, train loss: 0.004, val acc: 96.42%, val loss: 1.559\n",
            "epoch: 24, train acc: 99.96%, train loss: 0.002, val acc: 96.49%, val loss: 1.542\n",
            "epoch: 25, train acc: 99.97%, train loss: 0.001, val acc: 96.20%, val loss: 1.560\n",
            "epoch: 26, train acc: 100.00%, train loss: 0.000, val acc: 96.31%, val loss: 1.544\n",
            "epoch: 27, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.539\n",
            "epoch: 28, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.538\n",
            "epoch: 29, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.538\n",
            "epoch: 30, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.538\n",
            "epoch: 31, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.538\n",
            "epoch: 32, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 33, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 34, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.537\n",
            "epoch: 35, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.537\n",
            "epoch: 36, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 37, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 38, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 39, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 40, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 41, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 42, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 43, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.537\n",
            "epoch: 44, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 45, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 46, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 47, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 48, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 49, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 50, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 51, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 52, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 53, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 54, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 55, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 56, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 57, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 58, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 59, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 60, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 61, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 62, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 63, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 64, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 65, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 66, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 67, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 68, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 69, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 70, train acc: 100.00%, train loss: 0.000, val acc: 96.33%, val loss: 1.536\n",
            "epoch: 71, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 72, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 73, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 74, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 75, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 76, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 77, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 78, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 79, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 80, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 81, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 82, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 83, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 84, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 85, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 86, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 87, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 88, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 89, train acc: 100.00%, train loss: 0.000, val acc: 96.34%, val loss: 1.536\n",
            "epoch: 90, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 91, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 92, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 93, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 94, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 95, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 96, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 97, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 98, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 99, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n",
            "epoch: 100, train acc: 100.00%, train loss: 0.000, val acc: 96.35%, val loss: 1.536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEy3M_7LqvKt",
        "colab_type": "text"
      },
      "source": [
        "## Plot the training graph\n",
        "\n",
        "### *TODO: Include the following to your submission*\n",
        "Please attach the generated mnist_nn.png to your pdf. Compare the training and validation curves and explain your findings.\n",
        "\n",
        "Please export your notebook in ipynb format and submit it to Autolab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVlLVJTqq3NB",
        "colab_type": "code",
        "outputId": "27d56649-7c50-4543-b7fd-e8fd16f800f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "# plot loss and accuracy, and save the figure\n",
        "x_list = np.arange(1, n_epochs+1)\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(121)\n",
        "plt.plot(train_loss_list, label='training')\n",
        "plt.plot(val_loss_list, label='validation')\n",
        "plt.ylabel('loss', fontsize=16)\n",
        "plt.xlabel('num. epochs', fontsize=16)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(train_acc_list, label='training')\n",
        "plt.plot(val_acc_list, label='validation')\n",
        "plt.ylabel('accuracy', fontsize=16)\n",
        "plt.xlabel('num. epochs', fontsize=16)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('mnist_nn.png')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAEYCAYAAABBWFftAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILq292op4faH",
        "colab_type": "text"
      },
      "source": [
        "## LeNet-5\n",
        "\n",
        "Note that with a feed-forward neural network mentioned as an example above (2-layer NN, 800 HU, Cross-Entropy Loss), there are 784x800 + 800x10 + 800 + 10 = 636,010 parameters to train. Using Convolutional Neural Network, however, you can attain lower error rate using significantly less number of parameters. \n",
        "\n",
        "![LeNet](https://engmrk.com/wp-content/uploads/2018/09/LeNet_Original_Image.jpg)\n",
        "\n",
        "[LeNet-5: Original image contained in the publication. The architecture consists of two sets of convolutional, activation, and pooling layers, followed by a fully-connected layer, activation, another fully-connected, and finally a softmax classifier ([Image Source](https://engmrk.com/wp-content/uploads/2018/09/LeNet_Original_Image.jpg))]\n",
        "\n",
        "\n",
        "LeNet-5 is a 7-level Convolutional Neural Network first introduced in LeCun et al., 1998 with the following configuration:\n",
        "\n",
        "![LeNet-5 Architecture](https://engmrk.com/wp-content/uploads/2018/09/LeNEt_Summary_Table.jpg)\n",
        "\n",
        "Note that our input dimension is 28x28 while the input dimension of LeNet-5 is 32x32. One way to work around this is to zero-pad the input to make its dimension 32x32. \n",
        "\n",
        "While you are not required to implement any part of this network, we will ask you to compute the number of parameters in certain layers later in the assignment. For practice, you could compute the total number of trainable parameters in LeNet-5 and compare it to the number of parameters required for the network discussed above (2-layer NN, 800 HU, Cross-Entropy Loss).\n"
      ]
    }
  ]
}